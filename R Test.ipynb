{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: tm\n",
      "Loading required package: NLP\n"
     ]
    }
   ],
   "source": [
    "require(\"tm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: topicmodels\n"
     ]
    }
   ],
   "source": [
    "require(\"topicmodels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://github.com/RFJHaans/topicmodeling/blob/master/LDA200.RData?raw=true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'dtm' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'dtm' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load(url(\"https://github.com/RFJHaans/topicmodeling/blob/master/LDA200.RData?raw=true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'dtm' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'dtm' not found\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read.csv(url(\"https://raw.githubusercontent.com/RFJHaans/topicmodeling/master/ASQ_AMJ_AMR_OS_SMJ.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = VCorpus((VectorSource(data[, \"AB\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<<DocumentTermMatrix (documents: 1530, terms: 11744)>>\n",
       "Non-/sparse entries: 89755/17878565\n",
       "Sparsity           : 100%\n",
       "Maximal term length: 30\n",
       "Weighting          : term frequency (tf)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpusclean = tm_map(corpus, removeNumbers)\n",
    "# 2) Remove punctuation\n",
    "    corpusclean = tm_map(corpusclean, removePunctuation)\n",
    "# 3) Transform all upper-case letters to lower-case.\n",
    "    corpusclean = tm_map(corpusclean,  content_transformer(tolower))\n",
    "# 4) Remove stopwords which do not convey any meaning.\n",
    "    corpusclean = tm_map(corpusclean, removeWords, stopwords(\"english\"))\n",
    "# this stopword file is at C:\\Users\\[username]\\Documents\\R\\win-library\\[rversion]\\tm\\stopwords \n",
    "\n",
    "# i\tme\tmy\tmyself\twe\tour\tours\tourselves\tyou\tyour\tyours\tyourself\tyourselves\the\thim\this\thimself\t\n",
    "# she\ther\thers\therself\tit\tits\titself\tthey\tthem\ttheir\ttheirs\tthemselves\twhat\twhich\twho\twhom\tthis\n",
    "# that\tthese\tthose\tam\tis\tare\twas\twere\tbe\tbeen\tbeing\thave\thas\thad\thaving\tdo\tdoes\tdid\tdoing\twould\tshould\n",
    "# could\tought\ti'm\tyou're\the's\tshe's\tit's\twe're\tthey're\ti've\tyou've\twe've\tthey've\ti'd\tyou'd\the'd\tshe'd\twe'd\n",
    "# they'd\ti'll\tyou'll\the'll\tshe'll\twe'll\tthey'll\tisn't\taren't\twasn't\tweren't\thasn't\thaven't\thadn't\tdoesn't\t\n",
    "# don't\tdidn't\twon't\twouldn't\tshan't\tshouldn't\tcan't\tcannot\tcouldn't\tmustn't\tlet's\tthat's\twho's\twhat's\there's\n",
    "# there's\twhen's\twhere's\twhy's\thow's\ta\tan\tthe\tand\tbut\tif\tor\tbecause\tas\tuntil\twhile\tof\tat\tby\tfor\twith\tabout\t\n",
    "# against\tbetween\tinto\tthrough\tduring\tbefore\tafter\tabove\tbelow\tto\tfrom\tup\tdown\tin\tout\ton\toff\tover\tunder\tagain\n",
    "# further\tthen\tonce\there\tthere\twhen\twhere\twhy\thow\tall\tany\tboth\teach\tfew\tmore\tmost\tother\tsome\tsuch\tno\tnor\t\n",
    "# not\tonly\town\tsame\tso\tthan\ttoo\tvery\n",
    "\n",
    "# 5) And strip whitespace. \n",
    "    corpusclean = tm_map(corpusclean , stripWhitespace)\n",
    "\n",
    "# See the help of getTransformations for more possibilities, such as stemming. \n",
    "\n",
    "# To speed up the computation process for this tutorial, I have selected some choice words that were very common:\n",
    "# We update the corpusclean corpus by removing these words. \n",
    "    corpusclean = tm_map(corpusclean, removeWords, c(\"also\",\"based\",\"can\",\"data\",\"effect\",\n",
    "                                                  \"effects\",\"elsevier\",\"evidence\",\"examine\",\n",
    "                                                  \"find\",\"findings\",\"high\",\"low\",\"higher\",\"lower\",\n",
    "                                                  \"however\",\"impact\",\"implications\",\"important\",\n",
    "                                                  \"less\",\"literature\",\"may\",\"model\",\"one\",\"paper\",\n",
    "                                                  \"provide\",\"research\",\"all rights reserved\",\n",
    "                                                  \"results\",\"show\",\"studies\",\"study\",\"two\",\"use\",\n",
    "                                                  \"using\",\"rights\",\"reserved\",\"new\",\"analysis\",\"three\",\n",
    "                                                  \"associated\",\"firm\",\"firms\",\"copyright\",\"sons\",\"john\",\"ltd\",\"wiley\"))\n",
    "\n",
    "### Adding metadata from the original database\n",
    "# This needs to be done because transforming things into a corpus only uses the texts.\n",
    "    i = 0\n",
    "    corpusclean = tm_map(corpusclean, function(x) {\n",
    "      i <<- i +1\n",
    "      meta(x, \"id\") = as.character(data[i,\"ID\"])\n",
    "      x\n",
    "    })\n",
    "\n",
    "    i = 0\n",
    "    corpusclean = tm_map(corpusclean, function(x) {\n",
    "      i <<- i +1\n",
    "      meta(x, \"journal\") = as.character(data[i,\"SO\"])\n",
    "      x\n",
    "    })\n",
    "\n",
    "# The above is a loop that goes through all files (\"i\") in the corpus\n",
    "# and then maps the information of the metadata dataframe \n",
    "# (the \"ID\" column, et cetera) to a new piece of metadata in the corpus\n",
    "# which we also call \"id\", et cetera.\n",
    "\n",
    "# This enables making selections of the corpus based on metadata now.\n",
    "# Let's say we want to only look at articles from the AMJ, then we do the following:\n",
    "    keep = meta(corpusclean, \"journal\") == \"ACADEMY OF MANAGEMENT JOURNAL\"\n",
    "    corpus.AMJ = corpusclean[keep]\n",
    "\n",
    "# We then convert the corpus to a \"Document-term-matrix\" (dtm)\n",
    "    dtm =DocumentTermMatrix(corpusclean)  \n",
    "    dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
