#########################################
### Package installation
#########################################
# The following command loads the required packages.
library(topicmodels)
library(tm)
library(LDAvis)
library(igraph)
#########################################
### Data loading
#########################################
corpus_pr  <-Corpus(DirSource("Texts\\PR"), readerControl = list(reader=readPlain))
corpus_general  <-Corpus(DirSource("Texts\\General"), readerControl = list(reader=readPlain))
# Note that R loads up the texts in alphabetic order based on the filename, even though the folder itself
# was not sorted in this way. It is always good to check how the data are organized after loading to prevent
# errors in data combination further down the road.
# These files were read locally, but for ease of use, use the command below to load the starting file:
load(url("https://github.com/RFJHaans/topicmodeling/blob/master/Data/2018/Data_pr.RData?raw=true"))
load(url("https://github.com/RFJHaans/topicmodeling/blob/master/Data/2018/Data_general.RData?raw=true"))
#########################################
### Cleaning up the corpus
#########################################
# Below, we clean up the documents in various steps.
# We write everything to a new corpus called "corpusclean" so that we do not lose the original data.
# 1) Remove numbers
corpusclean_pr <- tm_map(corpus_pr, removeNumbers)
# 2) Remove punctuation
corpusclean_pr <- tm_map(corpusclean_pr, removePunctuation)
# 3) Transform all upper-case letters to lower-case.
corpusclean_pr <- tm_map(corpusclean_pr,  content_transformer(tolower))
# 4) Remove stopwords which do not convey any meaning.
corpusclean_pr <- tm_map(corpusclean_pr, removeWords, stopwords("english")) # this stopword file is at C:\Users\[username]\Documents\R\win-library\2.13\tm\stopwords
# 5) And strip whitespace.
corpusclean_pr <- tm_map(corpusclean_pr , stripWhitespace)
# 1) Remove numbers
corpusclean_general <- tm_map(corpus_general, removeNumbers)
# 2) Remove punctuation
corpusclean_general <- tm_map(corpusclean_general, removePunctuation)
# 3) Transform all upper-case letters to lower-case.
corpusclean_general <- tm_map(corpusclean_general,  content_transformer(tolower))
# 4) Remove stopwords which do not convey any meaning.
corpusclean_general <- tm_map(corpusclean_general, removeWords, stopwords("english")) # this stopword file is at C:\Users\[username]\Documents\R\win-library\2.13\tm\stopwords
# 5) And strip whitespace.
corpusclean_general <- tm_map(corpusclean_general , stripWhitespace)
# See the help of getTransformations for more possibilities, such as stemming.
#########################################
### More cleaning: infrequent words and frequent words
#########################################
# We convert the corpus to a "Document-term-matrix" (dtm)
dtm_pr <-DocumentTermMatrix(corpusclean_pr)
dtm_pr
dtm_general <-DocumentTermMatrix(corpusclean_general)
dtm_general
# dtms are organized with rows being documents and columns being the unique words.
# To speed up the computation process for this tutorial, I have selected some of the most frequent words
# that don't seem to be very meaningful.
# We update the corpusclean corpus by removing these words.
corpusclean_pr <- tm_map(corpusclean_pr, removeWords, c("words","also","can","get","going","just","well","said","will","thats","now","right","like","last","one","see"))
corpusclean_general <- tm_map(corpusclean_general, removeWords, c("words","also","can","get","going","just","well","said","will","thats","now","right","like","last","one","see"))
# We then create a dictionary that contains words occurring more than 50 times.
highfreq50_pr <- findFreqTerms(dtm_pr,50,Inf)
highfreq50_general <- findFreqTerms(dtm_general,50,Inf)
# and create a smaller dtm
# Note that this is completed on the corpus, not the DTM.
smalldtm_50w_pr <- DocumentTermMatrix(corpusclean_pr, control=list(dictionary = highfreq50_pr))
smalldtm_50w_general <- DocumentTermMatrix(corpusclean_general, control=list(dictionary = highfreq50_general))
#########################################
### LDA: Running the model
#########################################
# We first fix the random seed for future replication.
SEED <- 123456789
rm(corpus_general)
rm(corpus_pr)
save.image("C:/Users/rfjha/Desktop/temp.RData")
# 10 Topics
t1_10_pr <- Sys.time()
LDA10_pr <- LDA(smalldtm_50w_pr, k = 10, control = list(seed = SEED, verbose = 1))
t2_10_pr <- Sys.time()
t1_10_general <- Sys.time()
LDA10_general <- LDA(smalldtm_50w_general, k = 10, control = list(seed = SEED, verbose = 1))
t2_10_general <- Sys.time()
t2_10_pr- t1_10_pr
t2_10_general- t1_10_general
# 25 Topics
t1_25_pr <- Sys.time()
LDA25_pr <- LDA(smalldtm_50w_pr, k = 25, control = list(seed = SEED, verbose = 1))
t2_25_pr <- Sys.time()
t1_25_general <- Sys.time()
LDA25_general <- LDA(smalldtm_50w_general, k = 25, control = list(seed = SEED, verbose = 1))
t2_25_general <- Sys.time()
t2_25_pr- t1_25_pr
t2_25_general- t1_25_general
# 50 Topics
t1_50_pr <- Sys.time()
LDA50_pr <- LDA(smalldtm_50w_pr, k = 50, control = list(seed = SEED, verbose = 1))
t2_50_pr <- Sys.time()
t1_50_general <- Sys.time()
LDA50_general <- LDA(smalldtm_50w_general, k = 50, control = list(seed = SEED, verbose = 1))
t2_50_general <- Sys.time()
t2_50_pr- t1_50_pr
t2_50_general- t1_50_general
rm(corpusclean_general, corpusclean_pr)
rm(dtm_general, dtm_pr)
rm(smalldtm_50w_general,smalldtm_50w_pr)
save.image("~/GitHub/topicmodeling/Data/2018/Data_LDA.RData")
#########################################
### LDA: Rendering
#########################################
# We then create a variable that captures the top ten terms assigned to the 10-topic model:
topics_LDA10_pr <- terms(LDA10_pr, 10)
# And show the results:
topics_LDA10_pr
topics_LDA10_general <- terms(LDA10_general, 10)
# And show the results:
topics_LDA10_general
topics_LDA25_pr <- terms(LDA25_pr, 25)
# And show the results:
topics_LDA25_pr
topics_LDA25_general <- terms(LDA25_general, 25)
# And show the results:
topics_LDA25_general
topics_LDA50_pr <- terms(LDA50_pr, 50)
# And show the results:
topics_LDA50_pr
topics_LDA50_general <- terms(LDA50_general, 50)
# And show the results:
topics_LDA50_general
# Let's now create a file containing the topic loadings for all articles:
documents_LDA10_pr <- as.data.frame(LDA10_pr@gamma)
documents_LDA10_general <- as.data.frame(LDA10_general@gamma)
write.table(documents_LDA10_pr, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\documents_10_pr.csv", sep=',',row.names = FALSE)
write.table(documents_LDA10_general, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\documents_10_general.csv", sep=',',row.names = FALSE)
terms_LDA10_pr <- posterior(LDA10_pr)[["terms"]]
terms_LDA10_general <- posterior(LDA10_general)[["terms"]]
write.table(terms_LDA10_pr, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\terms_10_pr.csv", sep=',',row.names = FALSE)
write.table(terms_LDA10_general, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\terms_10_general.csv", sep=',',row.names = FALSE)
# If we are not necessarily interested in using the full range of topic loadings,
# but only in keeping those loadings that exceed a certain threshold,
# then we can use the code below:
topics(LDA10_pr, threshold = 0.2)
topics(LDA10_general, threshold = 0.2)
# Let's now create a file containing the topic loadings for all articles:
documents_LDA25_pr <- as.data.frame(LDA25_pr@gamma)
documents_LDA25_general <- as.data.frame(LDA25_general@gamma)
write.table(documents_LDA25_pr, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\documents_25_pr.csv", sep=',',row.names = FALSE)
write.table(documents_LDA25_general, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\documents_25_general.csv", sep=',',row.names = FALSE)
terms_LDA25_pr <- posterior(LDA25_pr)[["terms"]]
terms_LDA25_general <- posterior(LDA25_general)[["terms"]]
write.table(terms_LDA25_pr, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\terms_25_pr.csv", sep=',',row.names = FALSE)
write.table(terms_LDA25_general, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\terms_25_general.csv", sep=',',row.names = FALSE)
# If we are not necessarily interested in using the full range of topic loadings,
# but only in keeping those loadings that exceed a certain threshold,
# then we can use the code below:
topics(LDA25_pr, threshold = 0.2)
topics(LDA25_general, threshold = 0.2)
# Let's now create a file containing the topic loadings for all articles:
documents_LDA50_pr <- as.data.frame(LDA50_pr@gamma)
documents_LDA50_general <- as.data.frame(LDA50_general@gamma)
write.table(documents_LDA50_pr, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\documents_50_pr.csv", sep=',',row.names = FALSE)
write.table(documents_LDA50_general, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\documents_50_general.csv", sep=',',row.names = FALSE)
terms_LDA50_pr <- posterior(LDA50_pr)[["terms"]]
terms_LDA50_general <- posterior(LDA50_general)[["terms"]]
write.table(terms_LDA50_pr, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\terms_50_pr.csv", sep=',',row.names = FALSE)
write.table(terms_LDA50_general, file = "C:\\Users\\rfjha\\Documents\\GitHub\\topicmodeling\\Output\\2018\\terms_50_general.csv", sep=',',row.names = FALSE)
# If we are not necessarily interested in using the full range of topic loadings,
# but only in keeping those loadings that exceed a certain threshold,
# then we can use the code below:
topics(LDA50_pr, threshold = 0.2)
topics(LDA50_general, threshold = 0.2)
#####################################
### Creating topic networks (as in paper with Vern)
#####################################
post_10_pr <- topicmodels::posterior(LDA10_pr)
cor_mat_10_pr <- cor(t(post_10_pr[["terms"]]))
# Change row values to zero if less than row minimum plus row standard deviation
# This is how Jockers subsets the distance matrix to keep only
# closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/1036500)
cor_mat_10_pr[ sweep(cor_mat_10_pr, 1, (apply(cor_mat_10_pr,1,min) + 2*apply(cor_mat_10_pr,1,sd) )) < 0 ] <- 0
diag(cor_mat_10_pr) <- 0
g_10_pr <- igraph::graph.adjacency(cor_mat_10_pr, weighted = TRUE , mode= 'undirected')
plot(g_10_pr, layout = layout_with_kk,
edge.width=E(g_10_pr)$weight,
vertex.label.family = "sans",vertex.label.font=2, vertex.color = "white", vertex.label.color = "black")
post_10_general <- topicmodels::posterior(LDA10_general)
cor_mat_10_general <- cor(t(post_10_general[["terms"]]))
# Change row values to zero if less than row minimum plus row standard deviation
# This is how Jockers subsets the distance matrix to keep only
# closely related documents and avoid a dense spagetti diagram
# that's difficult to intergeneralet (hat-tip: http://stackoverflow.com/a/16047196/1036500)
cor_mat_10_general[ sweep(cor_mat_10_general, 1, (apply(cor_mat_10_general,1,min) + 2*apply(cor_mat_10_general,1,sd) )) < 0 ] <- 0
diag(cor_mat_10_general) <- 0
g_10_general <- igraph::graph.adjacency(cor_mat_10_general, weighted = TRUE , mode= 'undirected')
plot(g_10_general, layout = layout_with_kk,
edge.width=E(g_10_general)$weight,
vertex.label.family = "sans",vertex.label.font=2, vertex.color = "white", vertex.label.color = "black")
post_25_pr <- topicmodels::posterior(LDA25_pr)
cor_mat_25_pr <- cor(t(post_25_pr[["terms"]]))
# Change row values to zero if less than row minimum plus row standard deviation
# This is how Jockers subsets the distance matrix to keep only
# closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/2536500)
cor_mat_25_pr[ sweep(cor_mat_25_pr, 1, (apply(cor_mat_25_pr,1,min) + 2*apply(cor_mat_25_pr,1,sd) )) < 0 ] <- 0
diag(cor_mat_25_pr) <- 0
g_25_pr <- igraph::graph.adjacency(cor_mat_25_pr, weighted = TRUE , mode= 'undirected')
plot(g_25_pr, layout = layout_with_kk,
edge.width=E(g_25_pr)$weight,
vertex.label.family = "sans",vertex.label.font=2, vertex.color = "white", vertex.label.color = "black")
post_25_general <- topicmodels::posterior(LDA25_general)
cor_mat_25_general <- cor(t(post_25_general[["terms"]]))
# Change row values to zero if less than row minimum plus row standard deviation
# This is how Jockers subsets the distance matrix to keep only
# closely related documents and avoid a dense spagetti diagram
# that's difficult to intergeneralet (hat-tip: http://stackoverflow.com/a/16047196/2536500)
cor_mat_25_general[ sweep(cor_mat_25_general, 1, (apply(cor_mat_25_general,1,min) + 2*apply(cor_mat_25_general,1,sd) )) < 0 ] <- 0
diag(cor_mat_25_general) <- 0
g_25_general <- igraph::graph.adjacency(cor_mat_25_general, weighted = TRUE , mode= 'undirected')
plot(g_25_general, layout = layout_with_kk,
edge.width=E(g_25_general)$weight,
vertex.label.family = "sans",vertex.label.font=2, vertex.color = "white", vertex.label.color = "black")
post_50_pr <- topicmodels::posterior(LDA50_pr)
cor_mat_50_pr <- cor(t(post_50_pr[["terms"]]))
# Change row values to zero if less than row minimum plus row standard deviation
# This is how Jockers subsets the distance matrix to keep only
# closely related documents and avoid a dense spagetti diagram
# that's difficult to interpret (hat-tip: http://stackoverflow.com/a/16047196/5036500)
cor_mat_50_pr[ sweep(cor_mat_50_pr, 1, (apply(cor_mat_50_pr,1,min) + 2*apply(cor_mat_50_pr,1,sd) )) < 0 ] <- 0
diag(cor_mat_50_pr) <- 0
g_50_pr <- igraph::graph.adjacency(cor_mat_50_pr, weighted = TRUE , mode= 'undirected')
plot(g_50_pr, layout = layout_with_kk,
edge.width=E(g_50_pr)$weight,
vertex.label.family = "sans",vertex.label.font=2, vertex.color = "white", vertex.label.color = "black")
post_50_general <- topicmodels::posterior(LDA50_general)
cor_mat_50_general <- cor(t(post_50_general[["terms"]]))
# Change row values to zero if less than row minimum plus row standard deviation
# This is how Jockers subsets the distance matrix to keep only
# closely related documents and avoid a dense spagetti diagram
# that's difficult to intergeneralet (hat-tip: http://stackoverflow.com/a/16047196/5036500)
cor_mat_50_general[ sweep(cor_mat_50_general, 1, (apply(cor_mat_50_general,1,min) + 2*apply(cor_mat_50_general,1,sd) )) < 0 ] <- 0
diag(cor_mat_50_general) <- 0
g_50_general <- igraph::graph.adjacency(cor_mat_50_general, weighted = TRUE , mode= 'undirected')
plot(g_50_general, layout = layout_with_kk,
edge.width=E(g_50_general)$weight,
vertex.label.family = "sans",vertex.label.font=2, vertex.color = "white", vertex.label.color = "black")
###
# Using LDAVis
###
# https://gist.github.com/trinker/477d7ae65ff6ca73cace
#' Transform Model Output for Use with the LDAvis Package
topicmodels2LDAvis <- function(x, ...){
post <- topicmodels::posterior(x)
if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
mat <- x@wordassignments
LDAvis::createJSON(
phi = post[["terms"]],
theta = post[["topics"]],
vocab = colnames(post[["terms"]]),
doc.length = slam::row_sums(mat, na.rm = TRUE),
term.frequency = slam::col_sums(mat, na.rm = TRUE)
)
}
save.image("~/GitHub/topicmodeling/Data/2018/Data_LDA.RData")
